# -*- coding: utf-8 -*-
"""probabilistic Bayesian neural network.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bgcgIs6gZ8zBOvagL_SS21PyWPmE0659
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_datasets as tfds
import tensorflow_probability as tfp
from tensorflow.keras.layers import *


class BNN(object):
    def __init__(self):
        self.model = None
    
    # Define the prior weight distribution as Normal of mean=0 and stddev=1.
    # Note that, in this example, the we prior distribution is not trainable,
    # as we fix its parameters.
    def _prior(self,kernel_size, bias_size, dtype=None):
        n = kernel_size + bias_size
        prior_model = keras.Sequential(
            [
                tfp.layers.DistributionLambda(
                    lambda t: tfp.distributions.MultivariateNormalDiag(
                        loc=tf.zeros(n), scale_diag=tf.ones(n)
                    )
                )
            ]
        )
        return prior_model


    # Define variational posterior weight distribution as multivariate Gaussian.
    # Note that the learnable parameters for this distribution are the means,
    # variances, and covariances.
    def _posterior(self,kernel_size, bias_size, dtype=None):
        n = kernel_size + bias_size
        posterior_model = keras.Sequential(
            [
                tfp.layers.VariableLayer(
                    tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype
                ),
                tfp.layers.MultivariateNormalTriL(n),
            ]
        )
        return posterior_model



    def _negative_loglikelihood(self,targets, estimated_distribution):
        return -estimated_distribution.log_prob(targets)


    def fit(self,X,Y,validation_split=0,epochs=1,lr=0.001):
        '''
        lr: learning rate; float
        X: training examples; nparray
        Y: labels
        validation_split: ratio to split from (X,Y) as validation set
        epochs: number of epochs; int

        return: the trained model
        '''

        train_size = X.shape[0]
        input_length = X.shape[1]

        tf.keras.backend.clear_session()

        inputs = Input(shape=(input_length,))
        x = BatchNormalization()(inputs)
        x = tfp.layers.DenseVariational(units=8,make_prior_fn=self._prior,make_posterior_fn=self._posterior,kl_weight=1 / train_size,activation='sigmoid')(x)
        x = tfp.layers.DenseVariational(units=8,make_prior_fn=self._prior,make_posterior_fn=self._posterior,kl_weight=1 / train_size,activation='sigmoid')(x)
        distribution_params = layers.Dense(units=2)(x)
        outputs = tfp.layers.IndependentNormal(1)(distribution_params)


        model = tf.keras.Model(inputs=inputs,outputs=outputs)

        model.compile(
            optimizer=keras.optimizers.RMSprop(learning_rate=lr),
            loss=self._negative_loglikelihood,
            metrics=[keras.metrics.RootMeanSquaredError()],
        )

        Y = tf.cast(Y,tf.float32)

        model.fit(x=X,y=Y,epochs=epochs,validation_split=validation_split)
        self.model = model



    def predict(self,X):
        prediction_distribution = self.model(X)
        prediction_mean = prediction_distribution.mean().numpy().tolist()
        prediction_stdv = prediction_distribution.stddev().numpy()

        return prediction_mean,prediction_stdv





